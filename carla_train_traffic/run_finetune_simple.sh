#!/bin/bash
# ç®€åŒ–ç‰ˆå¾®è°ƒè„šæœ¬ - ä»…ä½¿ç”¨æ–°æ•°æ® + EWCé˜²é—å¿˜
# é€‚ç”¨äºŽæ²¡æœ‰æ—§æ•°æ®æˆ–æ—§æ•°æ®å¤ªå¤§çš„æƒ…å†µ

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
NUM_GPUS=6

# ============================================================
# é…ç½®åŒºåŸŸ - è¯·ä¿®æ”¹ä¸ºä½ çš„å®žé™…è·¯å¾„
# ============================================================

# é¢„è®­ç»ƒæ¨¡åž‹
PRETRAINED_MODEL="/path/to/your/best_model.pth"

# æ–°æ•°æ® (çº¢ç»¿ç¯åœºæ™¯)
NEW_TRAIN_DIR="/path/to/traffic_light/train"
NEW_EVAL_DIR="/path/to/traffic_light/val"

# ============================================================

export OMP_NUM_THREADS=4
export NCCL_IB_DISABLE=1

LOG_DIR="logs"
mkdir -p $LOG_DIR
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="${LOG_DIR}/finetune_simple_${TIMESTAMP}.log"

echo "============================================================"
echo "ðŸš¦ ç®€åŒ–ç‰ˆå¾®è°ƒ (ä»…æ–°æ•°æ® + EWC)"
echo "============================================================"
echo "æ—¥å¿—: $LOG_FILE"

# ä½¿ç”¨EWCé˜²é—å¿˜ï¼Œä¸éœ€è¦æ—§æ•°æ®
# ewc-lambda: è¶Šå¤§è¶Šä¿å®ˆï¼ŒæŽ¨è 1000-10000

torchrun --nproc_per_node=$NUM_GPUS --master_port=29501 finetune_anti_forget.py \
    --pretrained "$PRETRAINED_MODEL" \
    --new-train-dir "$NEW_TRAIN_DIR" \
    --new-eval-dir "$NEW_EVAL_DIR" \
    --batch-size 768 \
    --workers 6 \
    --lr 5e-5 \
    --epochs 30 \
    --ewc-lambda 5000 \
    --ewc-samples 3000 \
    --early-stop \
    --patience 8 \
    --channels-last \
    --id finetune_ewc_only \
    "$@" 2>&1 | tee $LOG_FILE
